# ğŸš€ Pipeline de DetecciÃ³n de Hilos en Conversaciones

Un sistema avanzado para detectar relaciones de respuesta implÃ­citas entre mensajes usando **tres modelos especializados** entrenados con datos enriquecidos de chats de Telegram.

---

## ğŸ“Š **Arquitectura del Sistema**

### ğŸ¯ **Tres Modelos Especializados**

#### 1. **Bi-Encoder A** ğŸï¸
- **Modelo**: `paraphrase-multilingual-mpnet-base-v2`
- **Fortaleza**: Excelente para comprensiÃ³n multilingÃ¼e y semÃ¡ntica profunda
- **Uso**: Genera embeddings de alta calidad para comparaciÃ³n semÃ¡ntica

#### 2. **Bi-Encoder B** âš¡
- **Modelo**: `sentence-transformers/all-MiniLM-L12-v2`
- **Fortaleza**: RÃ¡pido y eficiente, ideal para procesamiento en tiempo real
- **Uso**: Balance perfecto entre velocidad y precisiÃ³n

#### 3. **Cross-Encoder** ğŸ¯
- **Modelo**: `cross-encoder/ms-marco-MiniLM-L-12-v2`
- **Fortaleza**: MÃ¡xima precisiÃ³n en detecciÃ³n de relaciones contextuales
- **Uso**: AnÃ¡lisis profundo de pares de mensajes para casos difÃ­ciles

---

Perfecto. Te dejo **el README actualizado**, con:

### âœ” SecciÃ³n antigua reemplazada por la nueva explicaciÃ³n del sistema automÃ¡tico de selecciÃ³n de folds

### âœ” InclusiÃ³n de la explicaciÃ³n de los **modelos clÃ¡sicos** (GaussianNB, LogisticRegression, LightGBM, RandomForest)

### âœ” Integrado sin alterar la estructura del documento

### âœ” Estilo consistente con el resto del README

---

# ğŸš€ Pipeline de DetecciÃ³n de Hilos en Conversaciones (README Actualizado)

Un sistema avanzado para detectar relaciones de respuesta implÃ­citas entre mensajes usando **modelos neuronales**, **modelos clÃ¡sicos**, **selecciÃ³n inteligente de folds**, y **datos enriquecidos** a gran escala.

---

## ğŸ“Š **Arquitectura del Sistema**

### ğŸ¯ **Modelos Basados en Deep Learning**

#### 1. **Bi-Encoder A** ğŸï¸

* **Modelo**: `paraphrase-multilingual-mpnet-base-v2`
* **Uso**: Embeddings de semÃ¡ntica profunda
* **Fortaleza**: Excelente rendimiento multilingÃ¼e

#### 2. **Bi-Encoder B** âš¡

* **Modelo**: `sentence-transformers/all-MiniLM-L12-v2`
* **Uso**: Velocidad mÃ¡xima con buena precisiÃ³n
* **Fortaleza**: Ideal para despliegue rÃ¡pido

#### 3. **Cross-Encoder** ğŸ¯

* **Modelo**: `cross-encoder/ms-marco-MiniLM-L-12-v2`
* **Uso**: ClasificaciÃ³n contextual fina
* **Fortaleza**: MÃ¡xima precisiÃ³n en casos ambiguos

---

## ğŸ§® **Modelos ClÃ¡sicos Incluidos** 

AdemÃ¡s de los modelos neuronales, el pipeline ahora entrena **modelos clÃ¡sicos de Machine Learning** basados en las features extraÃ­das del dataset:

| Modelo                              | Motivo                                | Ventajas                                |
| ----------------------------------- | ------------------------------------- | --------------------------------------- |
| **GaussianNB**                      | Modelo muy ligero                     | RÃ¡pido, base line simple                |
| **Logistic Regression (L2 + SAGA)** | Clasificador lineal robusto           | Estable, rÃ¡pido, alta interpretabilidad |
| **LightGBM** (si estÃ¡ disponible)   | Ensamble basado en Ã¡rboles optimizado | Muy fuerte en features tabulares        |
| **Random Forest**                   | Modelo por consenso                   | Buen rendimiento y robusto al ruido     |

Cada uno se entrena en cada fold y se **guarda su progreso inmediatamente** en:

```
fold_X/classical_training_progress.json
```

Los modelos son exportados como:

```
gaussian_nb.joblib
logistic_regression.joblib
lightgbm.joblib
random_forest.joblib
```

Los resultados clÃ¡sicos **tambiÃ©n contribuyen a la comparaciÃ³n global del sistema**.

---

# ğŸ“˜ Sistema AutomÃ¡tico de SelecciÃ³n de Folds en el Entrenamiento 

Este mÃ³dulo decide automÃ¡ticamente **quÃ© esquema de validaciÃ³n cruzada** usar:

* **KFold tradicional**
* **GroupKFold**
* **Custom Stratified-Group-KFold** (avanzado)

La elecciÃ³n se basa en informaciÃ³n generada en:

```
threads_analysis/models/output/dataset_stats.json
```

---

## ğŸ“Š Â¿QuÃ© contiene `dataset_stats.json`?

El **Dataset Analyzer** calcula:

* NÃºmero total de ejemplos
* Conteos por clase globales y por chat
* Ratios:

  * `pos_ratio`
  * `neg_ratio`
  * `hard_ratio`
* TamaÃ±o por cada `chat_id`

Ejemplo:

```json
{
  "total_examples": 2894036,
  "labels": {...},
  "chats": {
    "13": {
      "total": 764518,
      "positive": 46205,
      "negative": 256962,
      "hard_negative": 461351,
      "pos_ratio": 0.0604,
      "neg_ratio": 0.3361,
      "hard_ratio": 0.6034
    }
  },
  "global_ratio": {
    "positive": 0.0562,
    "negative": 0.3823,
    "hard_negative": 0.5614
  }
}
```

---

# ğŸ§  LÃ³gica Inteligente de SelecciÃ³n de Folds

### ğŸ“Œ **Caso 1 â€” KFold simple**

Se usa cuando:

* No existe `chat_id`
* O solo existe un Ãºnico grupo

### ğŸ“Œ **Caso 2 â€” GroupKFold**

Se usa cuando:

* Hay suficientes chats (`n_groups >= 3 * n_splits`)
* La distribuciÃ³n entre chats es relativamente homogÃ©nea

Evita mezclar ejemplos de un mismo chat entre train/val.

### ğŸ“Œ **Caso 3 â€” Custom Stratified-Group-KFold**

Se usa cuando:

* Hay pocos chats
* **Pero sus distribuciones son extremadamente diferentes**

#### Â¿CÃ³mo funciona?

1. Para cada chat â†’ vector de perfil:

   ```
   [pos_ratio, neg_ratio, hard_ratio]
   ```
2. Se aplica **K-Means** â†’ 1 cluster por fold
3. Cada cluster representa un â€œtipo de chatâ€
4. Se garantiza que los folds tengan **ratios de clases equilibrados**

Este mÃ©todo es crucial cuando algunos chats son gigantes y otros pequeÃ±os, o cuando sus ratios varÃ­an enormemente.

---

# ğŸ§© Beneficios

* Evita fugas entre train/val
* Mantiene balance entre clases
* Reduce overfitting
* Funciona incluso con datasets muy desbalanceados
* 100% automÃ¡tico segÃºn el dataset real

---

# ğŸ¤– Entrenamiento de Modelos

Ahora incluye:

### âœ” 3 Modelos neuronales

### âœ” 4 Modelos clÃ¡sicos

### âœ” SelecciÃ³n inteligente de folds

### âœ” Reportes completos por fold

---

# ğŸ“¤ ExportaciÃ³n de Modelos

Deep Learning â†’ ONNX
ClÃ¡sicos â†’ `.joblib`

---

# ğŸ§ª EvaluaciÃ³n

La evaluaciÃ³n ahora incluye:

1. HeurÃ­sticas
2. Bi-Encoder A
3. Bi-Encoder B
4. Cross-Encoder
5. **GaussianNB**
6. **Logistic Regression**
7. **LightGBM** 
8. **RandomForest**

Cada uno con:

* F1
* Precision
* Recall
* AUC (modelos neuronales)

El reporte incluye todo:

```
evaluation_report.json
```

## ğŸ—ï¸ **Pipeline Completo**

### ğŸ”„ **Flujo de Procesamiento**

```mermaid
graph TD
    A[ğŸ“ Chats Telegram] --> B[ğŸ› ï¸ Dataset Builder]
    B --> C[ğŸ“Š Dataset Enriquecido]
    C --> D[ğŸ¤– Triple Model Trainer]
    D --> E[ğŸ“ˆ K-Fold Training]
    E --> F[ğŸ† Mejor Modelo]
    F --> G[ğŸ“¤ ExportaciÃ³n ONNX]
    G --> H[ğŸ§ª EvaluaciÃ³n]
    H --> I[ğŸ“‹ Reporte Final]
```

---

## ğŸ“ **Estructura del Proyecto**

```
threads_analysis/
â”œâ”€â”€ main.py                         # ğŸ  Punto de entrada para procesar chats y crear su grafo
â”œâ”€â”€ knowledge_graph.py              # ğŸ” ConstrucciÃ³n del grafo + heurÃ­sticas reply implÃ­cito
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dataset_builder.py          # ğŸ—ï¸ Construye dataset con hard negatives
â”‚   â”œâ”€â”€ model_trainer.py     # ğŸ¤– Entrena los modelos
â”‚   â”œâ”€â”€ onnx_export.py              # ğŸ“¤ Exporta modelos a ONNX
â”‚   â”œâ”€â”€ evaluation.py               # ğŸ§ª EvalÃºa vs heurÃ­sticas
â”‚   â”œâ”€â”€ pipeline_runner.py          # ğŸš€ Ejecuta TODO el pipeline
â”‚   â”œâ”€â”€ embeddings_cache/
â”‚   â”‚   â””â”€â”€ sentence-transformers__all-mpnet-base-v2/
â”‚   â”‚       â”œâ”€â”€ 0_meta.json         # ğŸ“‹ Metadatos del chat 0
â”‚   â”‚       â”œâ”€â”€ 0.npy               # ğŸ’¾ Embeddings del chat 0
â”‚   â”‚       â”œâ”€â”€ 1_meta.json         # ğŸ“‹ Metadatos del chat 1
â”‚   â”‚       â”œâ”€â”€ 1.npy               # ğŸ’¾ Embeddings del chat 1
â”‚   â”‚       â”œâ”€â”€ 2_meta.json         # ğŸ“‹ Metadatos del chat 2
â”‚   â”‚       â”œâ”€â”€ 2.npy               # ğŸ’¾ Embeddings del chat 2
â”‚   â”‚       â””â”€â”€ ...                 # ğŸ“Š MÃ¡s chats numerados
â”‚   â””â”€â”€ output/                     # ğŸ“¦ Salidas del pipeline
â”‚       â”œâ”€â”€ pairs_with_hard_neg.jsonl
â”‚       â”œâ”€â”€ pairs_with_hard_neg.jsonl.meta.json  # ğŸ“‹ Metadatos del dataset
â”‚       â”œâ”€â”€ training_summary.json   # ğŸ“Š Resumen entrenamiento
â”‚       â”œâ”€â”€ fold_0/                 # ğŸ¯ Modelos del fold 0
â”‚       â”‚   â”œâ”€â”€ bi_encoder_A_fold0.pth
â”‚       â”‚   â”œâ”€â”€ bi_encoder_B_fold0.pth
â”‚       â”‚   â””â”€â”€ cross_encoder_fold0.pth
â”‚       â”œâ”€â”€ fold_1/                 # ğŸ¯ Modelos del fold 1
â”‚       â”‚   â”œâ”€â”€ bi_encoder_A_fold1.pth
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ bi_encoder_A.onnx       # ğŸ“¤ Modelo exportado
â”‚       â”œâ”€â”€ bi_encoder_B.onnx       # ğŸ“¤ Modelo exportado
â”‚       â”œâ”€â”€ cross_encoder.onnx      # ğŸ“¤ Modelo exportado
â”‚       â””â”€â”€ evaluation_report.json  # ğŸ“ˆ Resultados evaluaciÃ³n
â””â”€â”€ threads_analysis_results/       # ğŸ“¥ Entrada de datos
    â””â”€â”€ train_chats/
        â”œâ”€â”€ chat_1.json             # ğŸ’¬ Chat de entrenamiento 1
        â”œâ”€â”€ chat_2.json             # ğŸ’¬ Chat de entrenamiento 2
        â””â”€â”€ ...                     # ğŸ’¬ MÃ¡s chats
```

## ğŸ¯ **CÃ³mo Ejecutar**

### ğŸš€ **Ejecutar Todo el Pipeline**
```bash
python threads_analysis/models/pipeline_runner.py
```

### âš™ï¸ **ParÃ¡metros Opcionales**
```bash
python threads_analysis/models/pipeline_runner.py \
    --train-chats "ruta/chats" \
    --output-dir "ruta/salida" \
    --epochs 4 \
    --batch-size 32 \
    --folds 5
```

---

## ğŸ—ï¸ **ConstrucciÃ³n del Dataset**

### ğŸ“Š **Proceso de Enriquecimiento**

#### ğŸ” **DetecciÃ³n de Pares Reales**
```python
# Para cada mensaje con reply_id vÃ¡lido
par_positivo = {
    'a': mensaje_padre,      # El mensaje referenciado
    'b': mensaje_actual,     # La respuesta
    'label': 1,              # âœ… Es una respuesta real
    'time_delta_min': 0.083, # 5 segundos en minutos
    'same_author': 0         # Â¿Mismo remitente?
}
```

#### ğŸ¯ **GeneraciÃ³n de Hard Negatives**
```python
# Busca mensajes SEMÃNTICAMENTE similares pero NO relacionados
hard_negative = {
    'a': mensaje_similar,    # Parecido semÃ¡ntico al mensaje real
    'b': mensaje_actual,
    'label': 0,              # âŒ NO es respuesta
    'hard_negative': True    # âš ï¸ Ejemplo difÃ­cil
}
```

#### ğŸ“ˆ **CaracterÃ­sticas ExtraÃ­das**
- **`tfidf_jaccard`**: Similitud de temas usando TF-IDF
- **`seq_ratio`**: Similitud textual directa
- **`emoji_diff`**: Diferencia en uso de emojis
- **`both_have_url`**: Ambos contienen enlaces
- **`both_all_caps`**: Ambos usan mayÃºsculas
- **`time_delta_min`**: Distancia temporal

---

## ğŸ¤– **Entrenamiento de Modelos**

### ğŸª **K-Fold Cross Validation**
```python
# Entrenamiento robusto con validaciÃ³n cruzada
for fold in range(5):  # 5 folds
    # 1. Fine-tune bi-encoders con MultipleNegativesRankingLoss
    # 2. Entrenar MLP classifier sobre embeddings
    # 3. Entrenar cross-encoder como clasificador binario
    # 4. Validar y guardar mÃ©tricas
```

### ğŸ† **SelecciÃ³n del Mejor Modelo**
- **MÃ©trica principal**: F1-Score en validaciÃ³n
- **SelecciÃ³n automÃ¡tica**: Mejor fold por modelo
- **Backup**: Todos los folds guardados

---

## ğŸ“¤ **ExportaciÃ³n ONNX**

### ğŸ¤” **Â¿QuÃ© es ONNX?**
**ONNX** (Open Neural Network Exchange) es un formato estÃ¡ndar para modelos de machine learning que permite:

### ğŸ¯ **Ventajas**
- **ğŸš€ Inferencia mÃ¡s rÃ¡pida**: Optimizado para producciÃ³n
- **ğŸ”§ Interoperabilidad**: Funciona en mÃºltiples frameworks
- **ğŸ“± Multiplataforma**: CPU, GPU, mÃ³viles, edge devices
- **âš¡ Sin dependencias**: No requiere PyTorch/TensorFlow en producciÃ³n

### ğŸ”„ **Proceso de ExportaciÃ³n**
```python
# Convierte modelo PyTorch a ONNX
torch.onnx.export(
    model,                      # Modelo entrenado
    dummy_input,               # Input de ejemplo
    "modelo.onnx",             # Archivo de salida
    opset_version=11,          # VersiÃ³n de operadores
    input_names=['input'],     # Nombres de inputs
    output_names=['output']    # Nombres de outputs
)
```

### ğŸ¯ **Â¿Por quÃ© Exportar a ONNX?**
1. **ğŸš€ Despliegue en producciÃ³n** sin dependencias pesadas
2. **ğŸ“± EjecuciÃ³n en dispositivos** con recursos limitados
3. **ğŸ”§ Compatibilidad** con mÃºltiples lenguajes (C++, Java, C#)
4. **âš¡ Optimizaciones** especÃ­ficas por hardware

---

## ğŸ§ª **EvaluaciÃ³n del Sistema**

### ğŸ“Š **MÃ©tricas de EvaluaciÃ³n**
- **AUC**: Ãrea bajo la curva ROC
- **PrecisiÃ³n**: Exactitud en predicciones positivas
- **Recall**: Capacidad de detectar todos los positivos
- **F1-Score**: Balance entre precisiÃ³n y recall

### ğŸ” **Comparativa Completa**
El sistema evalÃºa **4 enfoques**:

1. **ğŸ” HeurÃ­sticas Tradicionales**: Reglas basadas en knowledge graph
2. **ğŸï¸ Bi-Encoder A**: MPNet multilingÃ¼e para semÃ¡ntica profunda
3. **âš¡ Bi-Encoder B**: MiniLM para velocidad y eficiencia
4. **ğŸ¯ Cross-Encoder**: MÃ¡xima precisiÃ³n contextual

### ğŸ“ˆ **Reporte de EvaluaciÃ³n**
```json
{
  "heuristics": {"auc": 0.75, "f1": 0.68},
  "bi_encoder_A": {"auc": 0.89, "f1": 0.82},
  "bi_encoder_B": {"auc": 0.87, "f1": 0.80},
  "cross_encoder": {"auc": 0.92, "f1": 0.85}
}
```

---

## ğŸ“¦ **Salidas del Pipeline**

### âœ… **Archivos Generados**
- **`pairs_with_hard_neg.jsonl`**: Dataset completo enriquecido
- **`training_summary.json`**: Resumen de entrenamiento y mejores folds
- **`fold_*/model.pth`**: Modelos entrenados por cada fold
- **`*.onnx`**: Modelos exportados para producciÃ³n
- **`evaluation_report.json`**: Comparativa completa de rendimiento

### ğŸ¯ **Modelos ONNX Exportados**
1. **`bi_encoder_A.onnx`** - MPNet multilingÃ¼e optimizado
2. **`bi_encoder_B.onnx`** - MiniLM rÃ¡pido y eficiente
3. **`cross_encoder.onnx`** - MÃ¡xima precisiÃ³n contextual

---

## âš™ï¸ **Requisitos del Sistema**

### ğŸ“¦ **Dependencias**
```bash
pip install sentence-transformers torch scikit-learn numpy scipy tqdm pandas rich onnx onnxruntime
python -m spacy download es_core_news_md
```

### ğŸ“ **Estructura de Entrada**
Coloca tus chats en:
```
threads_analysis_results/train_chats/*.json
```

Ejemplo de estructura:
```json
{
  "metadata": {"chat_id": "grupo_1"},
  "messages": [
    {
      "id": 123,
      "text": "Â¿Alguien quiere pizza?",
      "reply_id": null,
      "date": "2023-10-01T10:00:00",
      "sender_id": "user1"
    }
  ]
}
```

---

## ğŸ¯ **Casos de Uso**

### ğŸ’¬ **DetecciÃ³n de Replies ImplÃ­citos**
```
Usuario A: "Â¿Alguien quiere pizza?"          ğŸ¯
Usuario B: "Â¡Yo sÃ­! Con pepperoni"           âœ… Respuesta detectada
Usuario C: "Acabo de almorzar"               âœ… Respuesta detectada  
Usuario D: "Hoy hace buen dÃ­a"               âŒ No relacionado
```

### ğŸ” **Aplicaciones**
- **ğŸ“Š AnÃ¡lisis de conversaciones**: Entender flujos de discusiÃ³n
- **ğŸ¤– Chatbots**: Mejorar comprensiÃ³n contextual
- **ğŸ“ˆ Business Intelligence**: Analizar patrones de comunicaciÃ³n
- **ğŸ” ModeraciÃ³n**: Detectar hilos de conversaciÃ³n problemÃ¡ticos

---

## ğŸ’¡ **Â¿Por quÃ© este Enfoque?**

### ğŸ† **Ventajas Clave**
- **ğŸ¯ PrecisiÃ³n MÃ¡xima**: CombinaciÃ³n de 3 modelos especializados
- **ğŸš€ Velocidad de Inferencia**: ONNX optimizado para producciÃ³n
- **ğŸ” Robustez**: Hard negatives + K-fold validation
- **ğŸ“Š EvaluaciÃ³n Completa**: Comparativa contra heurÃ­sticas existentes

### âš¡ **Ready for Production**
Los modelos ONNX exportados estÃ¡n listos para:
- **ğŸ¯ Despliegue inmediato** en entornos productivos
- **ğŸš€ Inferencia rÃ¡pida** sin dependencias de PyTorch
- **ğŸ“± EjecuciÃ³n eficiente** en mÃºltiples plataformas

---

**Â¿Listo para detectar relaciones en tus conversaciones? Â¡Ejecuta el pipeline y descubre insights ocultos! ...Solo si tienes 2 dÃ­as completos uno para hacer embeddings y otro para entrenar (sin gpu puede que mÃ¡s) y al menos estar dispuesto y con condiciones para bajar > 1.5 GB de espacio total que ocupan los modelos (sin contar los embbedings o el dataset que puede llegar a pesar mucho, con los datos usados peso ~1.4 GB) En fin, que divertidoooo... no me doliÃ³ en los datos mÃ³viles ni nada jaja... jaja... jaaaaaaa** ğŸ¥²ğŸ‰