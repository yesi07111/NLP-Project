# ðŸš€ Pipeline de DetecciÃ³n de Hilos en Conversaciones

Un sistema avanzado para detectar relaciones de respuesta implÃ­citas entre mensajes usando **tres modelos especializados** entrenados con datos enriquecidos de chats de Telegram.

---

## ðŸ“Š **Arquitectura del Sistema**

### ðŸŽ¯ **Tres Modelos Especializados**

#### 1. **Bi-Encoder A** ðŸŽï¸
- **Modelo**: `paraphrase-multilingual-mpnet-base-v2`
- **Fortaleza**: Excelente para comprensiÃ³n multilingÃ¼e y semÃ¡ntica profunda
- **Uso**: Genera embeddings de alta calidad para comparaciÃ³n semÃ¡ntica

#### 2. **Bi-Encoder B** âš¡
- **Modelo**: `sentence-transformers/all-MiniLM-L12-v2`
- **Fortaleza**: RÃ¡pido y eficiente, ideal para procesamiento en tiempo real
- **Uso**: Balance perfecto entre velocidad y precisiÃ³n

#### 3. **Cross-Encoder** ðŸŽ¯
- **Modelo**: `cross-encoder/ms-marco-MiniLM-L-12-v2`
- **Fortaleza**: MÃ¡xima precisiÃ³n en detecciÃ³n de relaciones contextuales
- **Uso**: AnÃ¡lisis profundo de pares de mensajes para casos difÃ­ciles

### ðŸ¤” **Â¿Por quÃ© Tres Modelos?**

| Modelo | Velocidad | PrecisiÃ³n | Caso de Uso |
|--------|-----------|-----------|-------------|
| Bi-Encoder A | ðŸŸ¡ Media | ðŸŸ¢ Alta | BÃºsqueda semÃ¡ntica multilingÃ¼e |
| Bi-Encoder B | ðŸŸ¢ Alta | ðŸŸ¡ Media | Filtrado rÃ¡pido y eficiente |
| Cross-Encoder | ðŸ”´ Baja | ðŸŸ¢ MÃ¡xima | DecisiÃ³n final en casos ambiguos |

---

## ðŸ—ï¸ **Pipeline Completo**

### ðŸ”„ **Flujo de Procesamiento**

```mermaid
graph TD
    A[ðŸ“ Chats Telegram] --> B[ðŸ› ï¸ Dataset Builder]
    B --> C[ðŸ“Š Dataset Enriquecido]
    C --> D[ðŸ¤– Triple Model Trainer]
    D --> E[ðŸ“ˆ K-Fold Training]
    E --> F[ðŸ† Mejor Modelo]
    F --> G[ðŸ“¤ ExportaciÃ³n ONNX]
    G --> H[ðŸ§ª EvaluaciÃ³n]
    H --> I[ðŸ“‹ Reporte Final]
```

---

## ðŸ“ **Estructura del Proyecto**

```
threads_analysis/
â”œâ”€â”€ knowledge_graph.py              # ðŸ” ConstrucciÃ³n del grafo + heurÃ­sticas reply implÃ­cito
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dataset_builder.py          # ðŸ—ï¸ Construye dataset con hard negatives
â”‚   â”œâ”€â”€ triple_model_trainer.py     # ðŸ¤– Entrena los 3 modelos
â”‚   â”œâ”€â”€ onnx_export.py              # ðŸ“¤ Exporta modelos a ONNX
â”‚   â”œâ”€â”€ evaluation.py               # ðŸ§ª EvalÃºa vs heurÃ­sticas
â”‚   â”œâ”€â”€ pipeline_runner.py          # ðŸš€ Ejecuta TODO el pipeline
â”‚   â”œâ”€â”€ embeddings_cache/
â”‚   â”‚   â””â”€â”€ sentence-transformers__all-mpnet-base-v2/
â”‚   â”‚       â”œâ”€â”€ 0_meta.json         # ðŸ“‹ Metadatos del chat 0
â”‚   â”‚       â”œâ”€â”€ 0.npy               # ðŸ’¾ Embeddings del chat 0
â”‚   â”‚       â”œâ”€â”€ 1_meta.json         # ðŸ“‹ Metadatos del chat 1
â”‚   â”‚       â”œâ”€â”€ 1.npy               # ðŸ’¾ Embeddings del chat 1
â”‚   â”‚       â”œâ”€â”€ 2_meta.json         # ðŸ“‹ Metadatos del chat 2
â”‚   â”‚       â”œâ”€â”€ 2.npy               # ðŸ’¾ Embeddings del chat 2
â”‚   â”‚       â””â”€â”€ ...                 # ðŸ“Š MÃ¡s chats numerados
â”‚   â””â”€â”€ output/                     # ðŸ“¦ Salidas del pipeline
â”‚       â”œâ”€â”€ pairs_with_hard_neg.jsonl
â”‚       â”œâ”€â”€ pairs_with_hard_neg.jsonl.meta.json  # ðŸ“‹ Metadatos del dataset
â”‚       â”œâ”€â”€ training_summary.json   # ðŸ“Š Resumen entrenamiento
â”‚       â”œâ”€â”€ fold_0/                 # ðŸŽ¯ Modelos del fold 0
â”‚       â”‚   â”œâ”€â”€ bi_encoder_A_fold0.pth
â”‚       â”‚   â”œâ”€â”€ bi_encoder_B_fold0.pth
â”‚       â”‚   â””â”€â”€ cross_encoder_fold0.pth
â”‚       â”œâ”€â”€ fold_1/                 # ðŸŽ¯ Modelos del fold 1
â”‚       â”‚   â”œâ”€â”€ bi_encoder_A_fold1.pth
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ bi_encoder_A.onnx       # ðŸ“¤ Modelo exportado
â”‚       â”œâ”€â”€ bi_encoder_B.onnx       # ðŸ“¤ Modelo exportado
â”‚       â”œâ”€â”€ cross_encoder.onnx      # ðŸ“¤ Modelo exportado
â”‚       â””â”€â”€ evaluation_report.json  # ðŸ“ˆ Resultados evaluaciÃ³n
â””â”€â”€ threads_analysis_results/       # ðŸ“¥ Entrada de datos
    â””â”€â”€ train_chats/
        â”œâ”€â”€ chat_1.json             # ðŸ’¬ Chat de entrenamiento 1
        â”œâ”€â”€ chat_2.json             # ðŸ’¬ Chat de entrenamiento 2
        â””â”€â”€ ...                     # ðŸ’¬ MÃ¡s chats
```

## ðŸŽ¯ **CÃ³mo Ejecutar**

### ðŸš€ **Ejecutar Todo el Pipeline**
```bash
python threads_analysis/models/pipeline_runner.py
```

### âš™ï¸ **ParÃ¡metros Opcionales**
```bash
python threads_analysis/models/pipeline_runner.py \
    --train-chats "ruta/chats" \
    --output-dir "ruta/salida" \
    --epochs 4 \
    --batch-size 32 \
    --folds 5
```

---

## ðŸ—ï¸ **ConstrucciÃ³n del Dataset**

### ðŸ“Š **Proceso de Enriquecimiento**

#### ðŸ” **DetecciÃ³n de Pares Reales**
```python
# Para cada mensaje con reply_id vÃ¡lido
par_positivo = {
    'a': mensaje_padre,      # El mensaje referenciado
    'b': mensaje_actual,     # La respuesta
    'label': 1,              # âœ… Es una respuesta real
    'time_delta_min': 0.083, # 5 segundos en minutos
    'same_author': 0         # Â¿Mismo remitente?
}
```

#### ðŸŽ¯ **GeneraciÃ³n de Hard Negatives**
```python
# Busca mensajes SEMÃNTICAMENTE similares pero NO relacionados
hard_negative = {
    'a': mensaje_similar,    # Parecido semÃ¡ntico al mensaje real
    'b': mensaje_actual,
    'label': 0,              # âŒ NO es respuesta
    'hard_negative': True    # âš ï¸ Ejemplo difÃ­cil
}
```

#### ðŸ“ˆ **CaracterÃ­sticas ExtraÃ­das**
- **`tfidf_jaccard`**: Similitud de temas usando TF-IDF
- **`seq_ratio`**: Similitud textual directa
- **`emoji_diff`**: Diferencia en uso de emojis
- **`both_have_url`**: Ambos contienen enlaces
- **`both_all_caps`**: Ambos usan mayÃºsculas
- **`time_delta_min`**: Distancia temporal

---

## ðŸ¤– **Entrenamiento de Modelos**

### ðŸŽª **K-Fold Cross Validation**
```python
# Entrenamiento robusto con validaciÃ³n cruzada
for fold in range(5):  # 5 folds
    # 1. Fine-tune bi-encoders con MultipleNegativesRankingLoss
    # 2. Entrenar MLP classifier sobre embeddings
    # 3. Entrenar cross-encoder como clasificador binario
    # 4. Validar y guardar mÃ©tricas
```

### ðŸ† **SelecciÃ³n del Mejor Modelo**
- **MÃ©trica principal**: F1-Score en validaciÃ³n
- **SelecciÃ³n automÃ¡tica**: Mejor fold por modelo
- **Backup**: Todos los folds guardados

---

## ðŸ“¤ **ExportaciÃ³n ONNX**

### ðŸ¤” **Â¿QuÃ© es ONNX?**
**ONNX** (Open Neural Network Exchange) es un formato estÃ¡ndar para modelos de machine learning que permite:

### ðŸŽ¯ **Ventajas**
- **ðŸš€ Inferencia mÃ¡s rÃ¡pida**: Optimizado para producciÃ³n
- **ðŸ”§ Interoperabilidad**: Funciona en mÃºltiples frameworks
- **ðŸ“± Multiplataforma**: CPU, GPU, mÃ³viles, edge devices
- **âš¡ Sin dependencias**: No requiere PyTorch/TensorFlow en producciÃ³n

### ðŸ”„ **Proceso de ExportaciÃ³n**
```python
# Convierte modelo PyTorch a ONNX
torch.onnx.export(
    model,                      # Modelo entrenado
    dummy_input,               # Input de ejemplo
    "modelo.onnx",             # Archivo de salida
    opset_version=11,          # VersiÃ³n de operadores
    input_names=['input'],     # Nombres de inputs
    output_names=['output']    # Nombres de outputs
)
```

### ðŸŽ¯ **Â¿Por quÃ© Exportar a ONNX?**
1. **ðŸš€ Despliegue en producciÃ³n** sin dependencias pesadas
2. **ðŸ“± EjecuciÃ³n en dispositivos** con recursos limitados
3. **ðŸ”§ Compatibilidad** con mÃºltiples lenguajes (C++, Java, C#)
4. **âš¡ Optimizaciones** especÃ­ficas por hardware

---

## ðŸ§ª **EvaluaciÃ³n del Sistema**

### ðŸ“Š **MÃ©tricas de EvaluaciÃ³n**
- **AUC**: Ãrea bajo la curva ROC
- **PrecisiÃ³n**: Exactitud en predicciones positivas
- **Recall**: Capacidad de detectar todos los positivos
- **F1-Score**: Balance entre precisiÃ³n y recall

### ðŸ” **Comparativa Completa**
El sistema evalÃºa **4 enfoques**:

1. **ðŸ” HeurÃ­sticas Tradicionales**: Reglas basadas en knowledge graph
2. **ðŸŽï¸ Bi-Encoder A**: MPNet multilingÃ¼e para semÃ¡ntica profunda
3. **âš¡ Bi-Encoder B**: MiniLM para velocidad y eficiencia
4. **ðŸŽ¯ Cross-Encoder**: MÃ¡xima precisiÃ³n contextual

### ðŸ“ˆ **Reporte de EvaluaciÃ³n**
```json
{
  "heuristics": {"auc": 0.75, "f1": 0.68},
  "bi_encoder_A": {"auc": 0.89, "f1": 0.82},
  "bi_encoder_B": {"auc": 0.87, "f1": 0.80},
  "cross_encoder": {"auc": 0.92, "f1": 0.85}
}
```

---

## ðŸ“¦ **Salidas del Pipeline**

### âœ… **Archivos Generados**
- **`pairs_with_hard_neg.jsonl`**: Dataset completo enriquecido
- **`training_summary.json`**: Resumen de entrenamiento y mejores folds
- **`fold_*/model.pth`**: Modelos entrenados por cada fold
- **`*.onnx`**: Modelos exportados para producciÃ³n
- **`evaluation_report.json`**: Comparativa completa de rendimiento

### ðŸŽ¯ **Modelos ONNX Exportados**
1. **`bi_encoder_A.onnx`** - MPNet multilingÃ¼e optimizado
2. **`bi_encoder_B.onnx`** - MiniLM rÃ¡pido y eficiente
3. **`cross_encoder.onnx`** - MÃ¡xima precisiÃ³n contextual

---

## âš™ï¸ **Requisitos del Sistema**

### ðŸ“¦ **Dependencias**
```bash
pip install sentence-transformers torch scikit-learn numpy scipy tqdm pandas rich onnx onnxruntime
python -m spacy download es_core_news_md
```

### ðŸ“ **Estructura de Entrada**
Coloca tus chats en:
```
threads_analysis_results/train_chats/*.json
```

Ejemplo de estructura:
```json
{
  "metadata": {"chat_id": "grupo_1"},
  "messages": [
    {
      "id": 123,
      "text": "Â¿Alguien quiere pizza?",
      "reply_id": null,
      "date": "2023-10-01T10:00:00",
      "sender_id": "user1"
    }
  ]
}
```

---

## ðŸŽ¯ **Casos de Uso**

### ðŸ’¬ **DetecciÃ³n de Replies ImplÃ­citos**
```
Usuario A: "Â¿Alguien quiere pizza?"          ðŸŽ¯
Usuario B: "Â¡Yo sÃ­! Con pepperoni"           âœ… Respuesta detectada
Usuario C: "Acabo de almorzar"               âœ… Respuesta detectada  
Usuario D: "Hoy hace buen dÃ­a"               âŒ No relacionado
```

### ðŸ” **Aplicaciones**
- **ðŸ“Š AnÃ¡lisis de conversaciones**: Entender flujos de discusiÃ³n
- **ðŸ¤– Chatbots**: Mejorar comprensiÃ³n contextual
- **ðŸ“ˆ Business Intelligence**: Analizar patrones de comunicaciÃ³n
- **ðŸ” ModeraciÃ³n**: Detectar hilos de conversaciÃ³n problemÃ¡ticos

---

## ðŸš€ **PrÃ³ximos Pasos**

1. **ðŸŽ¯ Despliegue en ProducciÃ³n**: Usar modelos ONNX exportados
2. **ðŸ“± API de Inferencia**: Servicio para detecciÃ³n en tiempo real
3. **ðŸ”§ Fine-tuning Continuo**: Mejora con nuevos datos
4. **ðŸŒ Multiplataforma**: EjecuciÃ³n en edge devices

---

## ðŸ’¡ **Â¿Por quÃ© este Enfoque?**

### ðŸ† **Ventajas Clave**
- **ðŸŽ¯ PrecisiÃ³n MÃ¡xima**: CombinaciÃ³n de 3 modelos especializados
- **ðŸš€ Velocidad de Inferencia**: ONNX optimizado para producciÃ³n
- **ðŸ” Robustez**: Hard negatives + K-fold validation
- **ðŸ“Š EvaluaciÃ³n Completa**: Comparativa contra heurÃ­sticas existentes

### âš¡ **Ready for Production**
Los modelos ONNX exportados estÃ¡n listos para:
- **ðŸŽ¯ Despliegue inmediato** en entornos productivos
- **ðŸš€ Inferencia rÃ¡pida** sin dependencias de PyTorch
- **ðŸ“± EjecuciÃ³n eficiente** en mÃºltiples plataformas

---

**Â¿Listo para detectar relaciones en tus conversaciones? Â¡Ejecuta el pipeline y descubre insights ocultos! ...Solo si tienes 2 dÃ­as completos uno para hacer embeddings y otro para entrenar (sin gpu puede que mÃ¡s) y al menos estar dispuesto y con condiciones para bajar > 1.5 GB de espacio total que ocupan los modelos (sin contar los embbedings o el dataset que puede llegar a pesar mucho, con los datos usados peso ~1.4 GB) En fin, que divertidoooo... no me doliÃ³ en los datos mÃ³viles ni anda jaja** ðŸŽ‰